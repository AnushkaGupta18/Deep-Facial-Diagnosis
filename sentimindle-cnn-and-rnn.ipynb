{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Neural Networks and Transformers:**","metadata":{}},{"cell_type":"markdown","source":"## **Convolutional Neural Networks:**","metadata":{}},{"cell_type":"code","source":"# dataframe and series \nimport pandas as pd\nimport numpy as np\n\n# sklearn imports for modeling part\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,balanced_accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom mlxtend.evaluate import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom mlxtend.plotting import plot_decision_regions\n\nfrom sklearn.metrics import confusion_matrix\n\n# To plot\nimport matplotlib.pyplot as plt  \n%matplotlib inline    \nimport matplotlib as mpl\nimport seaborn as sns\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:18:13.865557Z","iopub.execute_input":"2023-04-18T21:18:13.866182Z","iopub.status.idle":"2023-04-18T21:18:13.887665Z","shell.execute_reply.started":"2023-04-18T21:18:13.866143Z","shell.execute_reply":"2023-04-18T21:18:13.886634Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.__version__\nimport keras\nfrom keras import models, layers, optimizers\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.utils import pad_sequences\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Input, Flatten, BatchNormalization\nfrom keras.layers import RandomRotation, RandomTranslation, RandomFlip\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:18:13.890007Z","iopub.execute_input":"2023-04-18T21:18:13.890570Z","iopub.status.idle":"2023-04-18T21:18:13.902052Z","shell.execute_reply.started":"2023-04-18T21:18:13.890533Z","shell.execute_reply":"2023-04-18T21:18:13.900968Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_cnn = pd.read_csv('/kaggle/input/kindle-store-reviews-train-cnn/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:18:13.903512Z","iopub.execute_input":"2023-04-18T21:18:13.904331Z","iopub.status.idle":"2023-04-18T21:19:02.010380Z","shell.execute_reply.started":"2023-04-18T21:18:13.904295Z","shell.execute_reply":"2023-04-18T21:19:02.009312Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_cnn","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:19:02.013234Z","iopub.execute_input":"2023-04-18T21:19:02.013533Z","iopub.status.idle":"2023-04-18T21:19:02.033964Z","shell.execute_reply.started":"2023-04-18T21:19:02.013504Z","shell.execute_reply":"2023-04-18T21:19:02.032798Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"         overall  verified      reviewerID        asin  \\\n0            4.0      True  A25XR29KUK69PJ  B009QJMMKM   \n1            5.0      True  A32L1LC9TOB1YI  B00F53T9D6   \n2            5.0     False   A7KP9BA1UJGJ5  B00YQNI0RC   \n3            5.0      True   AW3TEBNYS6SFY  B001V9KG4E   \n4            5.0     False   AODTVO069QC8T  B018WHN5EU   \n...          ...       ...             ...         ...   \n1778053      4.0      True   A4K85P27BHGST  B01BJBXCVM   \n1778054      5.0      True   AY0LUGVU3EAKG  B00GX8U90E   \n1778055      2.0     False  A1JEFUO52AB8XH  B017WE2YUU   \n1778056      4.0     False  A2TQTPT4S772SD  B01GF3I5XY   \n1778057      5.0     False  A3UGX9FMI2BKPR  B01H4BYMC4   \n\n                                                reviewText  \\\n0        this is the first time i read the series and i...   \n1        who do you turn to when you are being threaten...   \n2        i received a copy of this book from the author...   \n3        stan morris - did a great job with this book: ...   \n4        we find smoke just where he flourishes best......   \n...                                                    ...   \n1778053  it started lik a mysterie and became a romance...   \n1778054  i've read both books, and i can say they were ...   \n1778055  turkey and miracle's storyline was the worst.....   \n1778056  this second volume of rave soup for the writer...   \n1778057  delilah is now running one of the more success...   \n\n                                             summary    style.Format:  \\\n0                                        Great Story   Kindle Edition   \n1                                     Great mystery!   Kindle Edition   \n2                             Absolutely wonderful!!   Kindle Edition   \n3                           Surviving the Fog Review   Kindle Edition   \n4                            Smoke Never Disappoints   Kindle Edition   \n...                                              ...              ...   \n1778053                                  Kiss is Fun   Kindle Edition   \n1778054                             Really Good Read   Kindle Edition   \n1778055                          Not what I expected   Kindle Edition   \n1778056  Rave Soup for the Writer's Soul: Volume Two   Kindle Edition   \n1778057                                   Danger Man   Kindle Edition   \n\n         sentiment                                       review_clean  \n0                2  this is the first time i read the series and i...  \n1                2  who do you turn to when you are being threaten...  \n2                2  i received a copy of this book from the author...  \n3                2  stan morris  did a great job with this book go...  \n4                2  we find smoke just where he flourishes bestaga...  \n...            ...                                                ...  \n1778053          2  it started lik a mysterie and became a romance...  \n1778054          2  ive read both books and i can say they were ve...  \n1778055          0  turkey and miracles storyline was the worst th...  \n1778056          2  this second volume of rave soup for the writer...  \n1778057          2  delilah is now running one of the more success...  \n\n[1778058 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overall</th>\n      <th>verified</th>\n      <th>reviewerID</th>\n      <th>asin</th>\n      <th>reviewText</th>\n      <th>summary</th>\n      <th>style.Format:</th>\n      <th>sentiment</th>\n      <th>review_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.0</td>\n      <td>True</td>\n      <td>A25XR29KUK69PJ</td>\n      <td>B009QJMMKM</td>\n      <td>this is the first time i read the series and i...</td>\n      <td>Great Story</td>\n      <td>Kindle Edition</td>\n      <td>2</td>\n      <td>this is the first time i read the series and i...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.0</td>\n      <td>True</td>\n      <td>A32L1LC9TOB1YI</td>\n      <td>B00F53T9D6</td>\n      <td>who do you turn to when you are being threaten...</td>\n      <td>Great mystery!</td>\n      <td>Kindle Edition</td>\n      <td>2</td>\n      <td>who do you turn to when you are being threaten...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>A7KP9BA1UJGJ5</td>\n      <td>B00YQNI0RC</td>\n      <td>i received a copy of this book from the author...</td>\n      <td>Absolutely wonderful!!</td>\n      <td>Kindle Edition</td>\n      <td>2</td>\n      <td>i received a copy of this book from the author...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>True</td>\n      <td>AW3TEBNYS6SFY</td>\n      <td>B001V9KG4E</td>\n      <td>stan morris - did a great job with this book: ...</td>\n      <td>Surviving the Fog Review</td>\n      <td>Kindle Edition</td>\n      <td>2</td>\n      <td>stan morris  did a great job with this book go...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>AODTVO069QC8T</td>\n      <td>B018WHN5EU</td>\n      <td>we find smoke just where he flourishes best......</td>\n      <td>Smoke Never Disappoints</td>\n      <td>Kindle Edition</td>\n      <td>2</td>\n      <td>we find smoke just where he flourishes bestaga...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1778053</th>\n      <td>4.0</td>\n      <td>True</td>\n      <td>A4K85P27BHGST</td>\n      <td>B01BJBXCVM</td>\n      <td>it started lik a mysterie and became a romance...</td>\n      <td>Kiss is Fun</td>\n      <td>Kindle Edition</td>\n      <td>2</td>\n      <td>it started lik a mysterie and became a romance...</td>\n    </tr>\n    <tr>\n      <th>1778054</th>\n      <td>5.0</td>\n      <td>True</td>\n      <td>AY0LUGVU3EAKG</td>\n      <td>B00GX8U90E</td>\n      <td>i've read both books, and i can say they were ...</td>\n      <td>Really Good Read</td>\n      <td>Kindle Edition</td>\n      <td>2</td>\n      <td>ive read both books and i can say they were ve...</td>\n    </tr>\n    <tr>\n      <th>1778055</th>\n      <td>2.0</td>\n      <td>False</td>\n      <td>A1JEFUO52AB8XH</td>\n      <td>B017WE2YUU</td>\n      <td>turkey and miracle's storyline was the worst.....</td>\n      <td>Not what I expected</td>\n      <td>Kindle Edition</td>\n      <td>0</td>\n      <td>turkey and miracles storyline was the worst th...</td>\n    </tr>\n    <tr>\n      <th>1778056</th>\n      <td>4.0</td>\n      <td>False</td>\n      <td>A2TQTPT4S772SD</td>\n      <td>B01GF3I5XY</td>\n      <td>this second volume of rave soup for the writer...</td>\n      <td>Rave Soup for the Writer's Soul: Volume Two</td>\n      <td>Kindle Edition</td>\n      <td>2</td>\n      <td>this second volume of rave soup for the writer...</td>\n    </tr>\n    <tr>\n      <th>1778057</th>\n      <td>5.0</td>\n      <td>False</td>\n      <td>A3UGX9FMI2BKPR</td>\n      <td>B01H4BYMC4</td>\n      <td>delilah is now running one of the more success...</td>\n      <td>Danger Man</td>\n      <td>Kindle Edition</td>\n      <td>2</td>\n      <td>delilah is now running one of the more success...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1778058 rows × 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_cnn.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:19:02.035819Z","iopub.execute_input":"2023-04-18T21:19:02.036787Z","iopub.status.idle":"2023-04-18T21:19:03.113035Z","shell.execute_reply.started":"2023-04-18T21:19:02.036745Z","shell.execute_reply":"2023-04-18T21:19:03.111895Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Dividing the data into training and test sets so as to train our model on random data. \n\ntrain_data, test_data = train_test_split(df_cnn, train_size=0.8, test_size=0.2,random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:19:03.115458Z","iopub.execute_input":"2023-04-18T21:19:03.115763Z","iopub.status.idle":"2023-04-18T21:19:04.034482Z","shell.execute_reply.started":"2023-04-18T21:19:03.115734Z","shell.execute_reply":"2023-04-18T21:19:04.033363Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_target = train_data.sentiment\ntrain_texts = train_data.review_clean\n\ntest_target = test_data.sentiment\ntest_texts = test_data.review_clean","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:19:04.036965Z","iopub.execute_input":"2023-04-18T21:19:04.037786Z","iopub.status.idle":"2023-04-18T21:19:04.047429Z","shell.execute_reply.started":"2023-04-18T21:19:04.037742Z","shell.execute_reply":"2023-04-18T21:19:04.046229Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### **Preparing data for Keras:**","metadata":{}},{"cell_type":"code","source":"def converting_texts(texts):\n    collected_texts = []\n    for text in texts:\n        collected_texts.append(text)\n    return collected_texts\n        \ntrain_texts = converting_texts(train_texts)\ntest_texts = converting_texts(test_texts)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:19:04.049073Z","iopub.execute_input":"2023-04-18T21:19:04.049653Z","iopub.status.idle":"2023-04-18T21:19:04.533321Z","shell.execute_reply.started":"2023-04-18T21:19:04.049612Z","shell.execute_reply":"2023-04-18T21:19:04.532261Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"I need to tokenize my text and padding sequences before modeling my data. I will use Keras proprocessing tools for this.","metadata":{}},{"cell_type":"code","source":"max_feat= 12000 #seting max features to define max number of tokenizer words\n\ntokenizer = Tokenizer(num_words=max_feat)\ntokenizer.fit_on_texts(train_texts)\n# updates internal vocabulary based on a list of texts\n# in the case where texts contains lists, we assume each entry of the lists to be a token\n# required before using texts_to_sequences or texts_to_matrix\n\ntrain_texts = tokenizer.texts_to_sequences(train_texts)\ntest_texts = tokenizer.texts_to_sequences(test_texts)\n# transforms each text in texts to a sequence of integers\n# Only top num_words-1 most frequent words will be taken into account \n# Only words known by the tokenizer will be taken into account","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:19:04.534875Z","iopub.execute_input":"2023-04-18T21:19:04.535302Z","iopub.status.idle":"2023-04-18T21:22:22.455823Z","shell.execute_reply.started":"2023-04-18T21:19:04.535251Z","shell.execute_reply":"2023-04-18T21:22:22.454783Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"max_len = max(len(train_ex) for train_ex in train_texts) #setting the max length\n\n# using pad_sequence tool from Keras\n# transforms a list of sequences to into a 2D Numpy array of shape \n# the maxlen argument for the length of the longest sequence in the list\ntrain_texts = pad_sequences(train_texts, maxlen=max_len)\ntest_texts = pad_sequences(test_texts, maxlen=max_len)\nprint(len(train_texts))\nprint(len(test_texts))","metadata":{"execution":{"iopub.status.busy":"2023-04-18T21:22:22.459475Z","iopub.execute_input":"2023-04-18T21:22:22.459890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To use batches productively, I need to turn my sequences to same length. I prefer to set everything to maximum length of the longest sentence in train data.","metadata":{}},{"cell_type":"markdown","source":"### Building a Model:\nIn this simple model, convolutional neural nets were used with 64 embedding dimension. 3-convolutional layers used, first two have batch normalization and maximum pooling arguments. The last one has glabal maximum pooling. Results were passed to a dense layer and output for prediction.\n\nBatch normalizations normalize and scale inputs or activations by reducing the amount what the hidden unit values shift around. Max Pool downsamples the input representation by taking the maximum value over the window defined by pool size.","metadata":{}},{"cell_type":"code","source":"def build_model():\n    sequences = layers.Input(shape=(max_len,))\n    embedded = layers.Embedding(max_feat, 64)(sequences)\n    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPool1D(3)(x)\n    x = layers.Conv1D(64, 5, activation='relu')(x)\n    x = layers.GlobalMaxPool1D()(x)\n    x = layers.Flatten()(x)\n    x = layers.Dense(100, activation='relu')(x)\n    predictions = layers.Dense(1, activation='sigmoid')(x)\n    model = models.Model(inputs=sequences, outputs=predictions)\n    model.compile(\n        optimizer='rmsprop',\n        loss='binary_crossentropy',\n        metrics=['binary_accuracy']\n    )\n    return model\n    \nmodel = build_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(\n    train_dataset, \n    batch_size = 128,\n    epochs=2,\n    validation_data=(test_dataset))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I ran the code in the data science lab because my laptop does not have a GPU, printing the outputs here: \n* loss_value = 0.28\n* binary accuracy = 0.91","metadata":{}},{"cell_type":"markdown","source":"## **Reccurent Neural Networks:**","metadata":{}},{"cell_type":"code","source":"def build_rnn_model():\n    sequences = layers.Input(shape=(max_len,))\n    embedded = layers.Embedding(max_feat, 64)(sequences)\n    x = layers.GRU(128, return_sequences=True)(embedded)\n    x = layers.GRU(128)(x)\n    x = layers.Dense(32, activation='relu')(x)\n    x = layers.Dense(100, activation='relu')(x)\n    predictions = layers.Dense(1, activation='sigmoid')(x)\n    model = models.Model(inputs=sequences, outputs=predictions)\n    model.compile(\n        optimizer='rmsprop',\n        loss='binary_crossentropy',\n        metrics=['binary_accuracy']\n    )\n    return model\n    \nrnn_model = build_rnn_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_model.fit(\n    train_texts, \n    train_target, \n    batch_size=128,\n    epochs=1,\n    validation_data=(test_texts, test_target) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I ran the code in the Data Science lab, and here are the outputs: \n* loss_value = 0.20\n* binary_accuracy = 0.92","metadata":{}}]}